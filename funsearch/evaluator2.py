"""Class for evaluating programs proposed by the Sampler."""
from funsearch.code_manipulation_2 import (
    Function,
    Program,
    header_from_str,
    structured_output_to_functions,
    structured_output_to_prog_meta
)

from funsearch.custom_types import FullName, FuncMeta

from funsearch import code_manipulation
from funsearch import programs_database_2
from funsearch import sandbox

from funsearch.structured_outputs import ProgramImplementation
from funsearch.test_case import TestCase

from typing import Any, Dict
from collections.abc import Sequence

import pathlib
import os

class ImplementationsManager:
    """Class that manages implementations of programs."""

    # Directory where implementations are saved
    implementations_root: pathlib.Path

    # This is a mapping from function full names to their metadata
    program_meta: dict[FullName, FuncMeta]
    
    @classmethod
    def _save_function(cls, function: Function, id: str):
        """
        Saves a single function to the implementation directory.
        
        Args:
            function (Function): The function to save.
            id (str): The unique identifier for the implementation.
        """
        func_file_path = cls.implementations_root / function.path
        func_code_path = func_file_path / (function.qualname + ' ' + id)

        os.makedirs(func_file_path, exist_ok=True)

        with open(func_code_path, "w") as file:
            file.write(function.body)

    @classmethod
    def save_implementation(cls, implementation: ProgramImplementation, id: str) -> Program:
        """Saves the implementation in the specified directory."""
        """Given sampler code in structured format, saves it in the implementation dir by current ID. Save in form accessible to decorator"""

        parsed_prog = structured_output_to_prog_meta(implementation, cls.program_meta)

        for function in parsed_prog.functions:
            cls._save_function(function, id)
        
        return parsed_prog


class AsyncEvaluator:
    """Class that analyses functions generated by LLMs."""

    def __init__(
        self,
        database: programs_database_2.ProgramsDatabase,
        sandbox: sandbox.ContainerSandbox,
        template: code_manipulation.Program,
        workspace: pathlib.Path,
        eval_file: pathlib.Path,
        imps_path: pathlib.Path,
        program_meta: dict[FullName, FuncMeta],
        tests: list[TestCase],
        timeout: float = 30.0,
    ):
        """
        Initializes the evaluator with the necessary components.

        Args:
            database (ProgramsDatabase): The database to store results.
            sbox (DummySandbox): The sandbox for running implementations.
            template (Program): The template program to use.
            workspace (Path): The workspace directory for saving implementations.
            eval_file (Path): The entropy point of the evaluator.
            imps_path (Path): Path where implementations are saved.
            program_meta (dict[FullName, FuncMeta]): Metadata for the program functions.
            tests (list[TestCase]): Test cases to run against the implementations.
            timeout (float): Timeout for each test case execution in seconds.
        """
        self._database = database
        self._template = template
        self._tests = tests
        self._timeout = timeout
        self._sandbox = sandbox
        self._eval_file = eval_file
        self._imps_path = imps_path
        self._program_meta = program_meta
        self._workspace = workspace

        sandbox.upload_test_cases(tests)

    async def analyse(
        self,
        implementation: ProgramImplementation,
        island_id: int | None,
        implementation_id: str
    ):
        """Compiles the sample into a program and executes it on test inputs."""
        program = ImplementationsManager.save_implementation(implementation, implementation_id)

        # Evaluate on the test cases and get the test scores
        test_scores = {}

        for test_id in range(len(self._tests)):
            test_output, runs_ok = self._sandbox.run(
                entry_point = self._eval_file,
                implementation_id = implementation_id,
                test_id = test_id,
                timeout = self._timeout,
            )

            if runs_ok and test_output is not None:
                if not isinstance(test_output, (int, float)):
                    raise ValueError("@run did not return an int/float score.")

                test_scores[self._tests[test_id]] = test_output

        if test_scores:
            self._database.register_program(program, island_id, test_scores)
