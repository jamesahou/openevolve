"""Class for evaluating programs proposed by the Sampler."""
from funsearch.code_manipulation_2 import (
    Function,
    Program,
    header_from_str,
    structured_output_to_functions,
)

from funsearch import code_manipulation
from funsearch import programs_database_2
from funsearch import sandbox

from funsearch.structured_outputs import ProgramImplementation
from funsearch.test_case import TestCase

from typing import Any, Tuple, List, Dict
from collections.abc import Sequence

import textwrap
import pathlib
import copy
import ast
import os
import re

class ImplementationsManager:
    """Class that manages implementations of programs."""

    implementations_root: pathlib.Path
    workspace: pathlib.Path
    program_meta: dict[str, Any]
    
    @classmethod
    def _save_function(cls, function: Function, id: str):
        """
        Saves a single function to the implementation directory.
        
        Args:
            function (Function): The function to save.
            id (str): The unique identifier for the implementation.
        """
        func_file_path = cls.implementations_root / function.path
        func_code_path = func_file_path / (function.qualname + ' ' + id)

        os.makedirs(func_file_path, exist_ok=True)

        with open(func_code_path, "w") as file:
            file.write(function.body)

    @classmethod
    def save_implementation(cls, implementation: ProgramImplementation, id: str) -> Program:
        """Saves the implementation in the specified directory."""
        """Given sampler code in structured format, saves it in the implementation dir by current ID. Save in form accessible to decorator"""

        # Ensure that the implementation has all the expected functions, and nothing extra
        implemented_function_qualnames = {function.qualname for function in implementation.functions}
        expected_function_qualnames = set(cls.program_meta.keys())

        missing_qualnames = expected_function_qualnames - implemented_function_qualnames
        extra_qualnames = implemented_function_qualnames - expected_function_qualnames
        
        if missing_qualnames or extra_qualnames:
            raise ValueError(
                f"Implemented functions do not match expected function names. "
                f"Missing: {missing_qualnames}, Extra: {extra_qualnames}"
            )

        functions = structured_output_to_functions(implementation)

        for function in functions.values():
            cls._save_function(function, id)
        
        return Program(functions=list(functions.values()))


class AsyncEvaluator:
    """Class that analyses functions generated by LLMs."""

    def __init__(
        self,
        database: programs_database_2.ProgramsDatabase,
        sbox: sandbox.DummySandbox,
        template: code_manipulation.Program,
        workspace: pathlib.Path,
        eval_file: pathlib.Path,
        imps_path: pathlib.Path,
        program_meta: Dict[str, Any],
        test_cases: Sequence[TestCase],
        timeout_seconds: int = 30,
    ):
        self._database = database
        self._template = template
        self._test_cases = test_cases
        self._timeout_seconds = timeout_seconds
        self._sandbox = sbox
        self._eval_file = eval_file
        self._imps_path = imps_path
        self._program_meta = program_meta
        self._workspace = workspace

    async def analyse(
        self,
        implementation: ProgramImplementation,
        island_id: int | None,
        implementation_id: str
    ):
        """Compiles the sample into a program and executes it on test inputs."""

        program = _save_sample(
            implementation, self._workspace, self._imps_path, self._program_meta, implementation_id
        )

        scores_per_test = {}

        for test_case in self._test_cases:
            test_output, runs_ok = self._sandbox.run(
                program, self._eval_file, test_case, self._timeout_seconds, curr_id
            )

            if runs_ok and test_output is not None:
                if not isinstance(test_output, (int, float)):
                    raise ValueError("@run did not return an int/float score.")
                    
                scores_per_test[test_case] = test_output

        if scores_per_test:
            self._database.register_program(program, island_id, scores_per_test)