"""Class for evaluating programs proposed by the Sampler."""
from funsearch.code_manipulation_2 import (
    Function,
    Program,
    header_from_str,
    structured_output_to_functions,
    structured_output_to_prog_meta
)

from funsearch.custom_types import FullName, FuncMeta

from funsearch import code_manipulation
from funsearch import programs_database_2
from funsearch import sandbox

from funsearch.structured_outputs import ProgramImplementation
from funsearch.test_case import TestCase

from typing import Any, Dict
from collections.abc import Sequence

import pathlib
import os

class ImplementationsManager:
    """Class that manages implementations of programs."""

    # Directory where implementations are saved
    implementations_root: pathlib.Path

    # This is a mapping from function full names to their metadata
    program_meta: dict[FullName, FuncMeta]
    
    @classmethod
    def _save_function(cls, function: Function, id: str):
        """
        Saves a single function to the implementation directory.
        
        Args:
            function (Function): The function to save.
            id (str): The unique identifier for the implementation.
        """
        func_file_path = cls.implementations_root / function.path
        func_code_path = func_file_path / (function.qualname + ' ' + id)

        os.makedirs(func_file_path, exist_ok=True)

        with open(func_code_path, "w") as file:
            file.write(function.body)

    @classmethod
    def save_implementation(cls, implementation: ProgramImplementation, id: str) -> Program:
        """Saves the implementation in the specified directory."""
        """Given sampler code in structured format, saves it in the implementation dir by current ID. Save in form accessible to decorator"""

        parsed_prog = structured_output_to_prog_meta(implementation, cls.program_meta)

        for function in parsed_prog.functions:
            cls._save_function(function, id)
        
        return parsed_prog


class AsyncEvaluator:
    """Class that analyses functions generated by LLMs."""

    def __init__(
        self,
        database: programs_database_2.ProgramsDatabase,
        sbox: sandbox.DummySandbox,
        template: code_manipulation.Program,
        workspace: pathlib.Path,
        eval_file: pathlib.Path,
        imps_path: pathlib.Path,
        program_meta: dict[FullName, FuncMeta],
        test_cases: Sequence[TestCase],
        timeout_seconds: int = 30,
    ):
        self._database = database
        self._template = template
        self._test_cases = test_cases
        self._timeout_seconds = timeout_seconds
        self._sandbox = sbox
        self._eval_file = eval_file
        self._imps_path = imps_path
        self._program_meta = program_meta
        self._workspace = workspace

    async def analyse(
        self,
        implementation: ProgramImplementation,
        island_id: int | None,
        implementation_id: str
    ):
        """Compiles the sample into a program and executes it on test inputs."""
        program = ImplementationsManager.save_implementation(implementation, implementation_id)

        # Evaluate on the test cases and get the test scores
        test_scores = {}

        for test_case in self._test_cases:
            test_output, runs_ok = self._sandbox.run(
                # TODO: What do we want to pass here?
                program, self._eval_file, test_case, self._timeout_seconds, implementation_id
            )

            if runs_ok and test_output is not None:
                if not isinstance(test_output, (int, float)):
                    raise ValueError("@run did not return an int/float score.")
                    
                test_scores[test_case] = test_output

        if test_scores:
            self._database.register_program(program, island_id, test_scores)