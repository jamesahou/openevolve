"""Class for evaluating programs proposed by the Sampler."""
from openevolve.code_manipulation import (
    Function,
    Program,
    header_from_str,
    structured_output_to_functions,
    structured_output_to_prog_meta
)

from openevolve.custom_types import FullName, FuncMeta

from openevolve import code_manipulation
from openevolve import programs_database
from openevolve import sandbox

from openevolve.structured_outputs import ProgramImplementation
from openevolve.test_case import TestCase

from typing import Any, Dict
from collections.abc import Sequence

import pathlib
import os

class ImplementationsManager:
    """Class that manages implementations of programs."""

    # Directory where the workspace is located
    workspace_root: pathlib.Path

    # Directory where implementations are saved
    implementations_root: pathlib.Path

    # This is a mapping from function full names to their metadata
    program_meta: dict[FullName, FuncMeta]

    @classmethod
    def set_workspace_root(cls, root: pathlib.Path):
        """
        Sets the root directory for the workspace.
        
        Args:
            root (Path): The root directory where the workspace will be located.
        """
        cls.workspace_root = root

    @classmethod
    def set_implementations_root(cls, root: pathlib.Path):
        """
        Sets the root directory for implementations.
        
        Args:
            root (Path): The root directory where implementations will be saved.
        """
        cls.implementations_root = root
    
    @classmethod
    def set_program_meta(cls, program_meta: dict[FullName, FuncMeta]):
        """
        Sets the metadata for the program functions.
        
        Args:
            program_meta (dict[FullName, FuncMeta]): Metadata for the program functions.
        """
        cls.program_meta = program_meta
    
    @classmethod
    def _save_function(cls, function: Function, id: str):
        """
        Saves a single function to the implementation directory.
        
        Args:
            function (Function): The function to save.
            id (str): The unique identifier for the implementation.
        """
        func_file_path = cls.implementations_root / function.path
        func_code_path = func_file_path / (function.qualname + ' ' + id)

        os.makedirs(func_file_path, exist_ok=True)

        with open(func_code_path, "w") as file:
            file.write(function.body)

    @classmethod
    def save_implementation(cls, implementation: ProgramImplementation, id: str) -> Program:
        """Saves the implementation in the specified directory."""
        """Given sampler code in structured format, saves it in the implementation dir by current ID. Save in form accessible to decorator"""
        parsed_prog = structured_output_to_prog_meta(implementation, cls.program_meta)

        for function in parsed_prog.functions:
            cls._save_function(function, id)
        
        return parsed_prog

class AsyncEvaluator:
    """Class that analyses functions generated by LLMs."""

    def __init__(
        self,
        database: programs_database.ProgramsDatabase,
        sandbox: sandbox.ContainerSandbox,
        tests: list[TestCase],
        timeout: float = 30.0,
    ):
        """
        Initializes the evaluator with the necessary components.

        Args:
            database (ProgramsDatabase): The database to store results.
            sandbox (DummySandbox): The sandbox for running implementations.
            tests (list[TestCase]): Test cases to run against the implementations.
            timeout (float): Timeout for each test case execution in seconds.
        """
        self.database = database
        self.tests = tests
        self.timeout = timeout
        self.sandbox = sandbox

        sandbox.upload_test_cases(tests)

    async def analyse(
        self,
        implementation: ProgramImplementation,
        island_id: int | None,
        implementation_id: str
    ):
        """Compiles the sample into a program and executes it on test inputs."""
        program = ImplementationsManager.save_implementation(implementation, implementation_id)

        # Evaluate on the test cases and get the test scores
        test_scores = {}

        for test_id in range(len(self._tests)):
            eval_result = self.sandbox.run(
                implementation_id = implementation_id,
                test_id = test_id,
                timeout = self.timeout,
            )

            runs_ok = eval_result.success
            test_output = eval_result.output

            if runs_ok and test_output is not None:
                if not isinstance(test_output, (int, float)):
                    raise ValueError("@run did not return an int/float score.")

                test_scores[self.tests[test_id]] = test_output

        if test_scores:
            self.database.register_program(program, island_id, test_scores)